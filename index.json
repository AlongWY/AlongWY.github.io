[{"contents":"ü§ó Datasets is a lightweight library providing two main features:\none-line dataloaders for many public datasets:\n one liners to download and pre-process any of the number of datasets major public datasets (in 467 languages and dialects!) provided on the HuggingFace Datasets Hub. With a simple command like squad_dataset = load_dataset(\u0026quot;squad\u0026quot;), get any of these datasets ready to use in a dataloader for training/evaluating a ML model (Numpy/Pandas/PyTorch/TensorFlow/JAX), efficient data pre-processing: simple, fast and reproducible data pre-processing for the above public datasets as well as your own local datasets in CSV/JSON/text. With simple commands like tokenized_dataset = dataset.map(tokenize_example), efficiently prepare the dataset for inspection and ML model evaluation and training.  ","description":"ü§ó The largest hub of ready-to-use datasets for ML models with fast, easy-to-use and efficient data manipulation tools","permalink":"https://alongwy.top/projects/contributions/huggingface-datasets/","tags":["NLP","Datasets"],"title":"Huggingface/datasets"},{"contents":"Intro An open-source neural language technology platform supporting six fundamental Chinese NLP tasks:\n lexical analysis (Chinese word segmentation, part-of-speech tagging, and named entity recognition) syntactic parsing (dependency parsing) semantic parsing (semantic dependency parsing and semantic role labeling).  Quickstart 1 2 3 4 5 6 7 8 9  from ltp import LTP ltp = LTP() # ÈªòËÆ§Âä†ËΩΩ Small Ê®°Âûã seg, hidden = ltp.seg([\u0026#34;‰ªñÂè´Ê±§ÂßÜÂéªÊãøÂ§ñË°£„ÄÇ\u0026#34;]) pos = ltp.pos(hidden) ner = ltp.ner(hidden) srl = ltp.srl(hidden) dep = ltp.dep(hidden) sdp = ltp.sdp(hidden)   Performance    Model CWS POS NER SRL DEP SDP Speed(Sents/S)     LTP 4.0 (Base) 98.70 98.50 95.4 80.60 89.50 75.20 39.12   LTP 4.0 (Base1) 99.22 98.73 96.39 79.28 89.57 76.57 \u0026ndash;.\u0026ndash;   LTP 4.0 (Base2) 99.18 98.69 95.97 79.49 90.19 76.62 \u0026ndash;.\u0026ndash;   LTP 4.0 (Small) 98.40 98.20 94.30 78.40 88.30 74.70 43.13   LTP 4.0 (Tiny) 96.80 97.10 91.60 70.90 83.80 70.10 53.22    Cite 1 2 3 4 5 6  @article{che2020n, title={N-LTP: A Open-source Neural Chinese Language Technology Platform with Pretrained Models}, author={Che, Wanxiang and Feng, Yunlong and Qin, Libo and Liu, Ting}, journal={arXiv preprint arXiv:2009.11616}, year={2020} }   ","description":"An open-source neural language technology platform supporting six   fundamental Chinese NLP tasks: \u003cul\u003e   \u003cli\u003elexical analysis (Chinese word segmentation,   part-of-speech tagging, and named entity recognition)\u003c/li\u003e   \u003cli\u003esyntactic parsing   (dependency parsing)\u003c/li\u003e   \u003cli\u003esemantic parsing (semantic dependency parsing and   semantic role labeling)\u003c/li\u003e \u003c/ul\u003e","permalink":"https://alongwy.top/projects/creations/language-technology-platform/","tags":["Python","Pytorch","NLP","Chinese"],"title":"Language Technology Platform"},{"contents":"An open-source neural language technology platform supporting six fundamental Chinese NLP tasks:\n lexical analysis (Chinese word segmentation, part-of-speech tagging, and named entity recognition) syntactic parsing (dependency parsing) semantic parsing (semantic dependency parsing and semantic role labeling).  Unlike the existing state-of-the-art toolkits, such as Stanza, that adopt an independent model for each task, N-LTP adopts the multi-task framework by using a shared pre-trained model, which has the advantage of capturing the shared knowledge across relevant Chinese tasks.\nIn addition, knowledge distillation where the single-task model teaches the multi-task model is further introduced to encourage the multi-task model to surpass its single-task teacher.\nFinally, we provide a collection of easy-to-use APIs and a visualization tool to make users easier to use and view the processing results directly. To the best of our knowledge, this is the first toolkit to support six Chinese NLP fundamental tasks.\n","description":"An open-source neural language technology platform supporting six   fundamental Chinese NLP tasks: \u003cul\u003e   \u003cli\u003elexical analysis (Chinese word segmentation,   part-of-speech tagging, and named entity recognition)\u003c/li\u003e   \u003cli\u003esyntactic parsing   (dependency parsing)\u003c/li\u003e   \u003cli\u003esemantic parsing (semantic dependency parsing and   semantic role labeling)\u003c/li\u003e \u003c/ul\u003e","permalink":"https://alongwy.top/publications/n-ltp-a-open-source-neural-chinese-language-technology-platform-with-pretrained-models/","tags":["EMNLP","Python","Pytorch","Demo","Chinese"],"title":"N-LTP: A Open-source Neural Chinese Language Technology Platform with Pretrained Models"},{"contents":"NotCraft::NotFeed An RSS reader running entirely from your GitHub repo.\n Free hosting on GitHub Pages. No ads. No third party tracking. No need for backend. Content updates via GitHub Actions. Customizable layouts and styles via templating and theming API. Just bring your HTML and CSS. Free and open source. No third-party tracking.  How to use it? Github Pages   Use the NotFeed-Template generate your own repository.\n  In the repository root, open Config.toml file, click the \u0026ldquo;Pencil (Edit this file)\u0026rdquo; button to edit.\n  Remove # to uncommend the cacheUrl property, replace \u0026lt;github_username\u0026gt; with your GitHub username, and replace \u0026lt;repo\u0026gt; with your GitHub repo name.\n  In the sources, update the items to the sources you want to follow. The final content of the file should look similar to this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  # Config.toml site_title = \u0026#34;ArxivDaily\u0026#34; cache_max_days = 7 sources = [ \u0026#34;https://export.arxiv.org/rss/cs.CL\u0026#34; ] # proxy = \u0026#34;http://127.0.0.1:7890\u0026#34; ## Optional: default is None # statics_dir = \u0026#34;statics\u0026#34; ## Optional: default is \u0026#34;statics\u0026#34; # templates_dir = \u0026#34;includes\u0026#34; ## Optional: default is \u0026#34;includes\u0026#34; # cache_url = \u0026#34;https://GITHUB_USERNAME.github.io/REPO_NAME/cache.json\u0026#34; # minify = true # [scripts] # highlight = \u0026#34;scripts/highlight.rhai\u0026#34;     Scroll to the bottom of the page, click \u0026ldquo;Commit changes\u0026rdquo; button.\n  Once the rebuild finishes, your feed will be available at https://\u0026lt;github_username\u0026gt;.github.io/\u0026lt;repo\u0026gt;\n  Localhost   Clone the NotFeed-Template repository.\n  Edit Config.toml file.\n  Run notfeed\n build: notfeed build serve: notfeed serve --addr 127.0.0.1 --port 8080 or simply notfeed serve    Thanks  Inspired by osmos::feed  ","description":"An RSS reader running entirely from your GitHub repo.  \u003cul\u003e   \u003cli\u003eFree hosting on GitHub Pages. No ads. No third party tracking. \u003c/li\u003e   \u003cli\u003eNo need for backend. Content updates via GitHub Actions.\u003c/li\u003e   \u003cli\u003eCustomizable layouts and styles via templating and theming API. Just bring your HTML and CSS. \u003c/li\u003e   \u003cli\u003eFree and open source. No third-party tracking.\u003c/li\u003e \u003c/ul\u003e","permalink":"https://alongwy.top/projects/creations/notfeed-a-rss-reader-on-github/","tags":["RSS","Rust"],"title":"NotFeed: A RSS Reader on GitHub"},{"contents":"This paper describes our submission system (HIT-SCIR) for the CoNLL 2020 shared task: Cross-Framework and Cross-Lingual Meaning Representation Parsing.\nThe task includes five frameworks for graph-based meaning representations, i.e., UCCA, EDS, PTG, AMR, and DRG.\nOur solution consists of two sub-systems:\n transition-based parser for Flavor (1) frameworks (UCCA, EDS, PTG) iterative inference parser for Flavor (2) frameworks (DRG, AMR).  In the final evaluation, our system is ranked 3rd among the seven team both in Cross-Framework Track and Cross-Lingual Track, with the macro-averaged MRP F1 score of 0.81/0.69.\n","description":"This paper describes our submission system (HIT-SCIR) for the   CoNLL 2020 shared task: Cross-Framework and Cross-Lingual Meaning   Representation Parsing.\u003c/br\u003e Our solution consists of two sub-systems: \u003cul\u003e   \u003cli\u003etransition-based parser for Flavor (1) frameworks   (UCCA, EDS, PTG)\u003c/li\u003e   \u003cli\u003eand iterative inference parser for Flavor (2) frameworks   (DRG, AMR)\u003c/li\u003e \u003c/ul\u003e In the final evaluation, our system is ranked 3rd among the seven team both in Cross-Framework Track and Cross-Lingual Track, with the macro-averaged MRP F1 score of 0.81/0.69.","permalink":"https://alongwy.top/publications/hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/","tags":["ACL","Python","Pytorch","NLP"],"title":"HIT-SCIR at MRP 2020: Transition-based Parser and Iterative Inference Parser"},{"contents":"gpustat  \nA rust version of gpustat.\nJust less than nvidia-smi?\nUsage $ gpustat\nOptions:\n --color : Force colored output (even when stdout is not a tty) --no-color : Suppress colored output -u, --show-user : Display username of the process owner -c, --show-cmd : Display the process name -f, --show-full-cmd : Display full command and cpu stats of running process -p, --show-pid : Display PID of the process -F, --show-fan : Display GPU fan speed -e, --show-codec : Display encoder and/or decoder utilization -a, --show-all : Display all gpu properties above  Quick Installation Install from Cargo:\n1  cargo install gpustat   Default display  [0] | A100-PCIE-40GB | 65\u0026rsquo;C | 75 % | 33409 / 40536 MB | along(33407M)\n  [0]: GPUindex (starts from 0) as PCI_BUS_ID A100-PCIE-40GB: GPU name 65'C: Temperature 75 %: Utilization 33409 / 40536 MB: GPU Memory Usage along(33407M): Username of the running processes owner on GPU (and their memory usage)  License GPL v2 License\n","description":"üìä A simple command-line utility for querying and monitoring GPU status","permalink":"https://alongwy.top/projects/creations/gpustat-a-rust-version-of-gpustat/","tags":["Rust","GPU","Nvidia"],"title":"gpustat: a rust-version of gpustat."},{"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n1 2  [outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;]   Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n1 2 3 4 5 6  ... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ...   Edit fuse.js options to Search static/js/search.js\n1 2 3 4 5 6  keys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ]   ","description":null,"permalink":"https://alongwy.top/search/","tags":null,"title":"Search Results"}]