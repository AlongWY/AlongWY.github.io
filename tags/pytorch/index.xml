<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Pytorch on 冯某人</title><link>https://alongwy.top/tags/pytorch/</link><description>Recent content in Pytorch on 冯某人</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 26 Aug 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://alongwy.top/tags/pytorch/index.xml" rel="self" type="application/rss+xml"/><item><title>Language Technology Platform</title><link>https://alongwy.top/projects/creations/ltp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alongwy.top/projects/creations/ltp/</guid><description>An open-source neural language technology platform supporting six fundamental Chinese NLP tasks: lexical analysis (Chinese word segmentation, part-of-speech tagging, and named entity recognition), syntactic parsing (dependency parsing), and semantic parsing (semantic dependency parsing and semantic role labeling). Unlike the existing state-of-the-art toolkits, such as Stanza, that adopt an independent model for each task, N-LTP adopts the multi-task framework by using a shared pre-trained model, which has the advantage of capturing the shared knowledge across relevant Chinese tasks.</description></item><item><title>N-LTP: A Open-source Neural Chinese Language Technology Platform with Pretrained Models</title><link>https://alongwy.top/publications/ltp/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://alongwy.top/publications/ltp/</guid><description>An open-source neural language technology platform supporting six fundamental Chinese NLP tasks: lexical analysis (Chinese word segmentation, part-of-speech tagging, and named entity recognition), syntactic parsing (dependency parsing), and semantic parsing (semantic dependency parsing and semantic role labeling). Unlike the existing state-of-the-art toolkits, such as Stanza, that adopt an independent model for each task, N-LTP adopts the multi-task framework by using a shared pre-trained model, which has the advantage of capturing the shared knowledge across relevant Chinese tasks.</description></item></channel></rss>