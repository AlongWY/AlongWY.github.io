<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on Feng</title><link>https://alongwy.top/</link><description>Recent content in Home on Feng</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 27 Aug 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://alongwy.top/index.xml" rel="self" type="application/rss+xml"/><item><title>Huggingface/datasets</title><link>https://alongwy.top/projects/contributions/huggingface-datasets/</link><pubDate>Sat, 28 Aug 2021 14:13:14 +0000</pubDate><guid>https://alongwy.top/projects/contributions/huggingface-datasets/</guid><description>ü§ó Datasets is a lightweight library providing two main features:
one-line dataloaders for many public datasets:
one liners to download and pre-process any of the number of datasets major public datasets (in 467 languages and dialects!) provided on the HuggingFace Datasets Hub. With a simple command like squad_dataset = load_dataset(&amp;quot;squad&amp;quot;), get any of these datasets ready to use in a dataloader for training/evaluating a ML model (Numpy/Pandas/PyTorch/TensorFlow/JAX), efficient data pre-processing: simple, fast and reproducible data pre-processing for the above public datasets as well as your own local datasets in CSV/JSON/text.</description></item><item><title>Language Technology Platform</title><link>https://alongwy.top/projects/creations/language-technology-platform/</link><pubDate>Sat, 28 Aug 2021 11:35:41 +0000</pubDate><guid>https://alongwy.top/projects/creations/language-technology-platform/</guid><description>Intro An open-source neural language technology platform supporting six fundamental Chinese NLP tasks:
lexical analysis (Chinese word segmentation, part-of-speech tagging, and named entity recognition) syntactic parsing (dependency parsing) semantic parsing (semantic dependency parsing and semantic role labeling). Quickstart 1 2 3 4 5 6 7 8 9 from ltp import LTP ltp = LTP() # ÈªòËÆ§Âä†ËΩΩ Small Ê®°Âûã seg, hidden = ltp.seg([&amp;#34;‰ªñÂè´Ê±§ÂßÜÂéªÊãøÂ§ñË°£„ÄÇ&amp;#34;]) pos = ltp.pos(hidden) ner = ltp.</description></item><item><title>N-LTP: A Open-source Neural Chinese Language Technology Platform with Pretrained Models</title><link>https://alongwy.top/publications/n-ltp-a-open-source-neural-chinese-language-technology-platform-with-pretrained-models/</link><pubDate>Sat, 28 Aug 2021 11:00:37 +0000</pubDate><guid>https://alongwy.top/publications/n-ltp-a-open-source-neural-chinese-language-technology-platform-with-pretrained-models/</guid><description>An open-source neural language technology platform supporting six fundamental Chinese NLP tasks:
lexical analysis (Chinese word segmentation, part-of-speech tagging, and named entity recognition) syntactic parsing (dependency parsing) semantic parsing (semantic dependency parsing and semantic role labeling). Unlike the existing state-of-the-art toolkits, such as Stanza, that adopt an independent model for each task, N-LTP adopts the multi-task framework by using a shared pre-trained model, which has the advantage of capturing the shared knowledge across relevant Chinese tasks.</description></item><item><title>gpustat: a rust-version of gpustat.</title><link>https://alongwy.top/projects/creations/gpustat-a-rust-version-of-gpustat/</link><pubDate>Sat, 28 Aug 2021 15:00:30 +0000</pubDate><guid>https://alongwy.top/projects/creations/gpustat-a-rust-version-of-gpustat/</guid><description>gpustat
A rust version of gpustat.
Just less than nvidia-smi?
Usage $ gpustat
Options:
--color : Force colored output (even when stdout is not a tty) --no-color : Suppress colored output -u, --show-user : Display username of the process owner -c, --show-cmd : Display the process name -f, --show-full-cmd : Display full command and cpu stats of running process -p, --show-pid : Display PID of the process -F, --show-fan : Display GPU fan speed -e, --show-codec : Display encoder and/or decoder utilization -a, --show-all : Display all gpu properties above Quick Installation Install from Cargo:</description></item><item><title>NotFeed: A RSS Reader on GitHub</title><link>https://alongwy.top/projects/creations/notfeed-a-rss-reader-on-github/</link><pubDate>Sat, 28 Aug 2021 11:40:32 +0000</pubDate><guid>https://alongwy.top/projects/creations/notfeed-a-rss-reader-on-github/</guid><description>NotCraft::NotFeed An RSS reader running entirely from your GitHub repo.
Free hosting on GitHub Pages. No ads. No third party tracking. No need for backend. Content updates via GitHub Actions. Customizable layouts and styles via templating and theming API. Just bring your HTML and CSS. Free and open source. No third-party tracking. How to use it? Github Pages Use the NotFeed-Template generate your own repository.
In the repository root, open Config.</description></item><item><title>HIT-SCIR at MRP 2020: Transition-based Parser and Iterative Inference Parser</title><link>https://alongwy.top/publications/hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/</link><pubDate>Tue, 01 Sep 2020 11:00:06 +0000</pubDate><guid>https://alongwy.top/publications/hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/</guid><description>This paper describes our submission system (HIT-SCIR) for the CoNLL 2020 shared task: Cross-Framework and Cross-Lingual Meaning Representation Parsing.
The task includes five frameworks for graph-based meaning representations, i.e., UCCA, EDS, PTG, AMR, and DRG.
Our solution consists of two sub-systems:
transition-based parser for Flavor (1) frameworks (UCCA, EDS, PTG) iterative inference parser for Flavor (2) frameworks (DRG, AMR). In the final evaluation, our system is ranked 3rd among the seven team both in Cross-Framework Track and Cross-Lingual Track, with the macro-averaged MRP F1 score of 0.</description></item><item><title>Search Results</title><link>https://alongwy.top/search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alongwy.top/search/</guid><description>This file exists solely to respond to /search URL with the related search layout template.
No content shown here is rendered, all content is based in the template layouts/page/search.html
Setting a very low sitemap priority will tell search engines this is not important content.
This implementation uses Fusejs, jquery and mark.js
Initial setup Search depends on additional output content type of JSON in config.toml
1 2 [outputs] home = [&amp;#34;HTML&amp;#34;, &amp;#34;JSON&amp;#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.</description></item></channel></rss>